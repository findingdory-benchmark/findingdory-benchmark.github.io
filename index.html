<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="FindingDory Benchmark for large-scale evalution of memory in VLM embodied agents.">
  <meta name="keywords" content="VLM, Embodied, Memory, Benchmark">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>FindingDory: A Benchmark to Evaluate Memory in Embodied Agents</title>

  <!-- Global site tag (gtag.js) - Google Analytics
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script> -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">FindingDory: A Benchmark to Evaluate Memory in Embodied Agents</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://www.karmeshyadav.com/">Karmesh Yadav<sup>*</sup></a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://yusufali98.github.io/">Yusuf Ali<sup>*</sup></a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://gunshigupta.netlify.app/">Gunshi Gupta</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.cs.ox.ac.uk/people/yarin.gal/website/">Yarin Gal</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://faculty.cc.gatech.edu/~zk15/">Zsolt Kira</a><sup>1</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Georgia Institute of Technology,</span>
            <span class="author-block"><sup>2</sup>Oxford University</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/google/nerfies"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://huggingface.co/datasets/yali30/findingdory"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
    <!-- Paper image. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Main Figure</h2>
        <div class="publication-image">
          <img src="./static/images/teaser.png" 
               alt="FindingDory Benchmark Overview Figure"
               style="width: 100%; height: auto;">
        </div>
      </div>
    </div>
    <!--/ Paper image. -->
  </div>
  
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Large vision-language models have recently demonstrated impressive performance in planning and control tasks, driving interest in their application to real-world robotics. 
            However, deploying these models for reasoning in embodied contexts is limited by their ability to incorporate long-term experience collected across multiple days and represented by vast collections of images. Current VLMs typically struggle to process more than a few hundred images concurrently, highlighting the need for more efficient mechanisms to handle long-term memory in embodied settings.
            To effectively evaluate these models for long-horizon control, a benchmark must specifically target scenarios where memory is crucial for success. Existing long-video QA benchmarks overlook embodied challenges like object manipulation and navigation, which demand low-level skills and fine-grained reasoning over past interactions. Moreover, effective memory integration in embodied agents involves both recalling relevant historical information and executing actions based on that information, making it essential to study these aspects together rather than in isolation.
            In this work, we introduce a new benchmark for long-range embodied tasks in the Habitat simulator. This benchmark evaluates memory-based capabilities across 57 tasks requiring sustained engagement and contextual awareness in an environment.  The tasks can also be procedurally extended to longer and more challenging versions, enabling scalable evaluation of memory and reasoning. We also present baselines that integrate state-of-the-art VLMs with low level navigation policies, assessing their performance on these memory-intensive tasks and highlight areas for improvement. Our benchmark code and dataset will be made publicly available.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Main Tasks. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Benchmark Tasks</h2>
        <div class="publication-image">
          <img src="./static/findingdory-website-images/all_tasks.png" 
               alt="FindingDory Benchmark Tasks"
               style="width: 100%; height: auto;">
        </div>
      </div>
    </div>
    <!--/ Main Tasks. -->
  </div>

</section>

<section class="section">
  <div class="container is-max-desktop">
    
    <!-- Results. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Results</h2>
      </div>
    </div>

    <!-- Main Results. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h3 class="title is-4">Main Results</h3>
        <div class="content has-text-justified">
          <p>
            Proprietary VLMs such as Gemini-2.0-flash and GPT-4o fail to perform appreciably across most FINDINGDORY task categories. Agents struggle significantly on multi-goal tasks where multiple subgoals are to be achieved sequentially based on the interaction videos collected during Phase 1 of the task (sometimes in a specific order). The Qwen-SFT baseline (see Appendix D.5) which is trained to predict ground-truth frame indices performs best but still reaches only â‰ˆ 50% on the high-level tasks. This highlights a considerable gap in the spatio-temporal reasoning capabilities of frontier models.          </p>
        </div>
        <div class="publication-image">
          <img src="./static/findingdory-website-images/main_results.png" 
               alt="Main results showing performance of different VLMs on FindingDory benchmark"
               style="width: 100%; height: auto;">
        </div>
      </div>
    </div>
    <!--/ Main Results. -->

    <!-- Video Subsampling Results. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h3 class="title is-4">Video Subsampling Results</h3>
        <div class="content has-text-justified">
          <p>
            We analyze how the number of input frames affects high-level performance by evaluating agents on subsampled interaction videos of varying lengths (Fig. 4a). This helps assess whether models benefit from longer histories or simply overfit to sparse visual cues. Interestingly, we observe that frozen VLMsâ€”despite being capable of accepting long contextsâ€”do not improve with more frames. In fact, their performance often degrades at higher frame counts (minimal subsampling), suggesting that they struggle to extract relevant information from densely packed, unfiltered input. This highlights a key limitation: naÂ¨Ä±vely increasing context length does not help unless the model can effectively attend to localized events. In contrast, the fine-tuned model (Qwen SFT) demon- strates clear gains when trained with longer videos, leverag- ing additional context to improve reasoning. This indicates that with appropriate supervision, models can move beyond shallow matching and utilize richer temporal signals from the interaction history.
          </p>
        </div>
        <div class="publication-image">
          <img src="./static/findingdory-website-images/subsampling_results.png" 
               alt="Video subsampling results comparing different sampling strategies"
               style="width: 100%; height: auto;">
        </div>
      </div>
    </div>
    <!--/ Video Subsampling Results. -->

    <!-- Low Level Policy Results. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h3 class="title is-4">Low Level Policy Results</h3>
        <div class="content has-text-justified">
          <p>
            When Qwen is combined with an ImageNav or Mapping agent, the overall SR and SPL reduces significantly (seen in the LL-SR/LL-SPL metrics)
          </p>
        </div>
        <div class="publication-image">
          <img src="./static/images/policy_results.png" 
               alt="Low level policy results showing performance comparison"
               style="width: 100%; height: auto;">
        </div>
      </div>
    </div>
    <!--/ Low Level Policy Results. -->

  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    
    <!-- Qualitative Results. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Qualitative Results</h2>
      </div>
    </div>

    <!-- Example Video Sequence. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h3 class="title is-4">Example Video Sequence</h3>
        <div class="content has-text-justified">
          <p>
            Example Video Sequence Collected during Phase 1 of FindingDory task
          </p>
        </div>
        <div class="publication-video">
          <video id="qualitative-video" controls muted loop playsinline width="100%">
            <source src="./static/findingdory-website-images/ep_id_327.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
    <!--/ Example Video Sequence. -->

    <!-- Agent Trajectory. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h3 class="title is-4">Agent Trajectory</h3>
        <div class="content has-text-justified">
          <p>
            Bird's eye view of the agent trajectory showing the path taken during the various pick-place interaction routines.
          </p>
        </div>
        <div class="publication-image">
          <img src="./static/findingdory-website-images/agent_trajectory.png" 
               alt="Bird's eye view of agent trajectory during FindingDory task"
               style="width: 100%; height: auto;">
        </div>
      </div>
    </div>
    <!--/ Agent Trajectory. -->

  </div>
</section>



<!-- 
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{park2021nerfies,
  author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
  title     = {Nerfies: Deformable Neural Radiance Fields},
  journal   = {ICCV},
  year      = {2021},
}</code></pre>
  </div>
</section> -->


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            Website template adapted from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
